# Data & Data Processing Deep Dive
## Diplomacy Lie Detection Dataset Analysis

---

## Table of Contents
1. [Dataset Overview](#dataset-overview)
2. [Data Sources](#data-sources)
3. [Data Processing Pipeline](#data-processing-pipeline)
4. [Data Structures & Examples](#data-structures--examples)
5. [Data Quality & Statistics](#data-quality--statistics)
6. [Key Processing Decisions](#key-processing-decisions)
7. [Data Challenges & Solutions](#data-challenges--solutions)

---

## Dataset Overview

### What is This Dataset?

The "It Takes Two to Lie" dataset captures **strategic deception in naturalistic game settings** using the board game Diplomacy. Unlike laboratory experiments, this data comes from real players engaged in multi-hour games where deception is a legitimate strategy.

### Key Characteristics

- **17,289 total messages** across 12 complete games
  - Train: 13,132 messages (9 games)
  - Validation: 1,416 messages (1 game)
  - Test: 2,741 messages (2 games)
- **591 lies (4.5%)** and **12,541 truths (95.5%)** in Train split
- **Dual annotations**: Both sender intent (actual lie) and receiver perception (suspected lie)
- **Rich context**: Game moves, power dynamics, temporal information
- **Multiple formats**: JSONL (original), ConvoKit corpus (utterance-level), Move files (game state)

### Why This Data Matters

1. **Naturalistic**: Players are genuinely motivated to win
2. **Strategic**: Deception serves a purpose (alliance-building, betrayal, misdirection)
3. **Annotated**: Both sender and receiver perspectives captured
4. **Contextual**: Full game state available for each message

---

## Data Sources

The POC pipeline integrates three data sources:

### 1. ConvoKit Corpus (Primary)
- **Format**: Utterance-level conversation corpus
- **Access**: Downloaded automatically via `convokit.download("diplomacy-corpus")`
- **Structure**: Conversations ‚Üí Utterances with metadata
- **Splits**: Train (13,132 messages, 9 games), Validation (1,416 messages, 1 game), Test (2,741 messages, 2 games)
- **Split Level**: Game-level (each complete game belongs to exactly one split)

### 2. Original JSONL Files (Alternative)
- **Location**: `data/train.jsonl`, `data/validation.jsonl`, `data/test.jsonl`
- **Format**: One game per line, nested structure
- **Content**: Complete game dialogs with all annotations

### 3. Move Files (Game Context)
- **Location**: `moves/DiplomacyGame{ID}_{YEAR}_{SEASON}.json`
- **Count**: 342 files (12 games √ó ~28.5 turns average)
- **Content**: Orders for all players in each turn with results

---

## Data Processing Pipeline

### Pipeline Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Step 1: Data Ingestion                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ConvoKit Corpus          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ  - 246 conversations  ‚îÄ‚îÄ‚îÄ>‚îÇ   Extract    ‚îÇ                   ‚îÇ
‚îÇ  - Train/Val/Test splits  ‚îÇ  Utterances  ‚îÇ‚îÄ‚îÄ> cleaned_       ‚îÇ
‚îÇ  (17,289 total messages)  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    messages.csv   ‚îÇ
‚îÇ                                                (POC: 13,132   ‚îÇ
‚îÇ                                                 Train only)   ‚îÇ
‚îÇ  Move Files (342)         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ  - Game/Year/Season   ‚îÄ‚îÄ‚îÄ>‚îÇ  Index by    ‚îÇ                   ‚îÇ
‚îÇ  - Player orders          ‚îÇ Game Context ‚îÇ‚îÄ‚îÄ> moves_data.json‚îÇ
‚îÇ                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    (1,787 entries)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ             Step 2: Baseline Metrics Calculation             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Cleaned Messages         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ  - Labels                ‚îÄ>‚îÇ  Calculate   ‚îÇ‚îÄ‚îÄ> Statistics    ‚îÇ
‚îÇ  - Metadata               ‚îÇ  Deception   ‚îÇ    - Rates       ‚îÇ
‚îÇ                           ‚îÇ  Patterns    ‚îÇ    - Quadrants   ‚îÇ
‚îÇ                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    - Temporal    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Step 1: Data Ingestion in Detail

#### 1.1 ConvoKit Extraction

```python
# Load corpus
corpus = Corpus(filename=download("diplomacy-corpus"))

# Filter by split (Train/Validation/Test)
train_corpus = corpus.filter_conversations_by(
    lambda convo: convo.meta.get('acl2020_fold') == 'Train'
)

# Extract utterances
for conversation in train_corpus.get_conversation_ids():
    conversation = corpus.get_conversation(conversation_id)
    for utterance_id in conversation.get_utterance_ids():
        utterance = corpus.get_utterance(utterance_id)
        # Extract message, metadata, labels
```

**What's Extracted:**
- Message text
- Sender & receiver
- Speaker intention (Lie/Truth) ‚Üí Binary label (1/0)
- Receiver perception
- Game metadata (year, score, delta)
- Conversation context

#### 1.2 Move Data Loading

```python
# For each move file: DiplomacyGame1_1901_spring.json
for move_file in moves_dir.glob("DiplomacyGame*.json"):
    # Parse filename
    game_id = 1
    year = "1901"
    season = "spring"
    
    # Load move content
    move_content = json.load(move_file)
    
    # Index by (game_id, year, season) ‚Üí all moves
    moves_data[(game_id, year, season)] = move_content
    
    # Further index by player
    for player, orders in move_content['orders'].items():
        move_index[(game_id, year, season, player)] = orders
```

**Result:** Fast lookup of player moves for any message context

---

## Data Structures & Examples

### Cleaned Messages CSV

**Schema:**
```
message                  : string  (raw message text)
sender                   : string  (e.g., "italy-Game1")
receiver                 : string  (e.g., "germany-Game1")
label                    : int     (0=Truth, 1=Lie)
conversation_id          : string  (e.g., "Game1-italy-germany")
absolute_message_index   : int     (position in entire game)
relative_message_index   : int     (position in this conversation)
year                     : int     (1901-1918)
game_score               : int     (supply centers owned)
game_score_delta         : int     (sender score - receiver score)
deception_quadrant       : string  (Straightforward/Deceived/Caught/Cassandra/Unknown)
split                    : string  (Train/Validation/Test)
receiver_perception      : string  (Truth/Lie/empty)
game_id                  : float   (NaN - extracted from sender instead)
```

**Example Row 1: Truthful Message**
```csv
message: "Germany! Just the person I want to speak with..."
sender: italy-Game1
receiver: germany-Game1
label: 0  (Truth)
conversation_id: Game1-italy-germany
absolute_message_index: 74
relative_message_index: 0
year: 1901
game_score: 3
game_score_delta: 0
deception_quadrant: Straightforward
split: Train
receiver_perception: Truth
```

**Analysis:** Italy sends a truthful opening message to Germany. Both players have 3 supply centers (equal power). Receiver perceives it as truthful (Straightforward quadrant).

**Example Row 2: Deceptive Message**
```csv
message: "I don't think I'm ready to go for that idea, however I'd be down for some good ol'-fashioned Austria-kicking?"
sender: germany-Game1
receiver: italy-Game1
label: 0  (Truth - sender marked as truth)
conversation_id: Game1-italy-germany
absolute_message_index: 119
relative_message_index: 8
year: 1901
game_score: 3
game_score_delta: 0
deception_quadrant: Cassandra
split: Train
receiver_perception: Lie  (Receiver thinks it's a lie!)
```

**Analysis:** Germany sends what they mark as truthful, but Italy perceives it as deceptive (Cassandra quadrant - truthful message suspected as lie).

**Example Row 3: Caught Liar**
```csv
message: [Some deceptive claim about intentions]
sender: italy-Game1
receiver: england-Game1
label: 1  (Lie)
conversation_id: Game1-italy-england
year: 1902
game_score: 4
game_score_delta: 1  (Italy has 1 more supply center)
deception_quadrant: Caught
split: Train
receiver_perception: Lie
```

**Analysis:** Italy lies and England catches it. Italy has a slight power advantage. Both sender and receiver agree it's deceptive.

### Deception Quadrants Explained

The dataset categorizes messages into 5 quadrants based on sender intent vs receiver perception:

| Quadrant | Sender Intent | Receiver Perception | Count | Lie % |
|----------|---------------|---------------------|-------|-------|
| **Straightforward** | Truth | Truth | 10,979 | 0.00% |
| **Unknown** | Truth/Lie | No annotation | 1,107 | 4.16% |
| **Cassandra** | Truth | Lie | 501 | 0.00% |
| **Deceived** | Lie | Truth | 480 | 100% |
| **Caught** | Lie | Lie | 65 | 100% |

**Key Insights:**
- **Straightforward**: Most common (83.6%) - honest communication
- **Deceived**: Successful lies (3.7%) - deception goes undetected
- **Caught**: Failed lies (0.5%) - deception detected
- **Cassandra**: Unjustified suspicion (3.8%) - truthful but doubted
- **Unknown**: Missing receiver annotation (8.4%)

### Move Data Structure

**File:** `moves/DiplomacyGame1_1901_spring.json`

```json
{
  "sc": "Austria 3\nEngland 3\nFrance 3\nGermany 3\nItaly 3\nRussia 4\nTurkey 3",
  "orders": {
    "Italy": {
      "Ven": {
        "type": "HOLD",
        "result": "SUCCEEDS",
        "result_reason": "Unchallenged"
      },
      "Nap": {
        "to": "ION",
        "type": "MOVE",
        "result": "SUCCEEDS",
        "result_reason": "Attack strength is greater"
      },
      "Rom": {
        "to": "Apu",
        "type": "MOVE",
        "result": "SUCCEEDS",
        "result_reason": "Attack strength is greater"
      }
    },
    "Germany": {
      "Ber": {
        "to": "Kie",
        "type": "MOVE",
        "result": "SUCCEEDS"
      },
      "Mun": {
        "type": "HOLD",
        "result": "SUCCEEDS"
      }
    }
    // ... other countries
  },
  "territories": {
    "Par": "France",
    "Ven": "Italy",
    // ... all territories
  }
}
```

**Structure Breakdown:**
- `sc`: Supply center counts (raw HTML string from game)
- `orders`: Dict of player ‚Üí unit ‚Üí order details
- `territories`: Current territory ownership

**Order Types:**
- `MOVE`: Unit moves to adjacent territory
- `HOLD`: Unit stays in place
- `SUPPORT`: Unit supports another's move
- `CONVOY`: Fleet transports army

**Results:**
- `SUCCEEDS`: Order executed successfully
- `FAILS`: Order blocked or invalid
- `result_reason`: Explanation (e.g., "Attack strength is greater")

### Processed Move Index Structure

**File:** `poc/data/moves_data.json`

```json
{
  "moves_data": {
    "(1, '1901', 'spring')": {
      "sc": "...",
      "orders": { ... }
    }
  },
  "move_index": {
    "(1, '1901', 'spring', 'italy')": {
      "Ven": { "type": "HOLD", "result": "SUCCEEDS" },
      "Nap": { "to": "ION", "type": "MOVE", "result": "SUCCEEDS" },
      "Rom": { "to": "Apu", "type": "MOVE", "result": "SUCCEEDS" }
    }
  }
}
```

**Key Features:**
- Tuple keys converted to strings for JSON serialization
- Fast lookup by (game_id, year, season, player)
- 1,787 player-specific move entries
- 342 full game turn entries

---

## Data Quality & Statistics

### Message-Level Statistics

```
Total Messages:           17,289
‚îú‚îÄ Train:                 13,132 (76.0%) - 9 games
‚îú‚îÄ Validation:             1,416 (8.2%) - 1 game
‚îî‚îÄ Test:                   2,741 (15.9%) - 2 games

Labels (Train split):
‚îú‚îÄ Truths (0):            12,541 (95.5%)
‚îî‚îÄ Lies (1):                 591 (4.5%)

Class Imbalance Ratio:    21.22:1 (truth:lie)

Note: POC pipeline currently processes Train split only.
Full dataset statistics available but not yet calculated.
```

### Temporal Distribution

Deception rates vary significantly by game year:

```
Year   Messages   Lies   Rate
--------------------------------
1901    3,131     175    5.59%  ‚Üê Highest early game
1902    2,933      92    3.14%
1903    1,548      64    4.13%
1904    1,226      38    3.10%
1905    1,430      23    1.61%  ‚Üê Lowest mid-game
1906      858      50    5.83%
1907    1,133     118   10.41%  ‚Üê Highest late game!
1908      397      20    5.04%
1909      288      10    3.47%
1910      188       1    0.53%
```

**Key Patterns:**
- **Early game (1901)**: Higher deception (5.59%) - alliance formation, bluffing
- **Mid-game (1905)**: Lower deception (1.61%) - alliances stabilized
- **Late game (1907)**: Highest deception (10.41%) - betrayals, endgame tactics
- **Very late (1910)**: Minimal deception (0.53%) - game nearly over

### Sender-Receiver Pair Analysis

Top 10 most active pairs:

```
Sender              Receiver            Messages  Lies   Lie %
----------------------------------------------------------------
england-Game1    ‚Üí  germany-Game1          380      0    0.00%
austria-Game2    ‚Üí  italy-Game2            336      5    1.49%
italy-Game2      ‚Üí  austria-Game2          320      0    0.00%
germany-Game2    ‚Üí  england-Game2          305      0    0.00%
germany-Game1    ‚Üí  england-Game1          295      9    3.05%
austria-Game2    ‚Üí  germany-Game2          257     12    4.67%
england-Game3    ‚Üí  germany-Game3          252      4    1.59%
italy-Game1      ‚Üí  england-Game1          249     49   19.68%  ‚Üê Highly deceptive!
italy-Game2      ‚Üí  germany-Game2          236      1    0.42%
germany-Game2    ‚Üí  austria-Game2          223      6    2.69%
```

**Key Insight:** Italy‚ÜíEngland in Game1 shows **19.68% deception rate** - much higher than average (4.5%). This suggests a particularly deceptive relationship or strategic betrayal.

### Power Dynamics

```
Game Score (Supply Centers):
‚îú‚îÄ Mean:    5.21 centers
‚îú‚îÄ Range:   0-18 centers
‚îî‚îÄ Typical: 3-7 centers (starting is 3)

Game Score Delta (Sender - Receiver):
‚îú‚îÄ Mean:    0.07 (nearly balanced)
‚îú‚îÄ Range:   -14 to +14
‚îî‚îÄ Typical: -1 to +1
```

**Observation:** Most messages occur between roughly equal-power players (delta ‚âà 0).

### Move Data Coverage

```
Move Files:                342
Move Index Entries:      1,787 (player-specific)
Average Turns per Game:   28.5

Message-to-Move Matching (Train split):
‚îú‚îÄ Matched:              13,128 (99.97%)
‚îî‚îÄ Missing:                   4 (0.03%)
```

**Excellent coverage** - almost all messages have corresponding move context.

### Split Structure Details

**Game-to-Split Mapping:**

| Split | Games | Conversations | Messages | Percentage |
|-------|-------|---------------|----------|------------|
| Train | Game1, 2, 3, 5, 6, 7, 8, 9, 10 | 184 | 13,132 | 76.0% |
| Validation | Game11 | 20 | 1,416 | 8.2% |
| Test | Game4, 12 | 42 | 2,741 | 15.9% |
| **Total** | **12 games** | **246** | **17,289** | **100%** |

**Key Characteristics:**
- Each game has ~21 conversations (one per player pair: 7 choose 2)
- All conversations from a game stay in the same split
- No data leakage between Train/Validation/Test
- Game-level split prevents learning from other conversations in same game

**Verification:**
```
‚úì ConvoKit corpus: 17,289 messages total
‚úì Original JSONL files: 17,289 messages total  
‚úì All messages have valid binary labels (Lie/Truth)
‚úì No messages filtered out due to missing annotations
```

---

## Key Processing Decisions

### 1. Data Source: ConvoKit vs JSONL

**Decision:** Use ConvoKit corpus as primary data source

**Rationale:**
- Utterance-level format easier to work with
- Built-in train/validation/test splits
- Clean API for conversations and metadata
- Widely used in NLP research

**Trade-off:**
- Slightly different structure than original JSONL
- Additional dependency

### 2. Split Structure

**Organization:** Game-level splits (each game belongs to exactly one split)

**Split Distribution:**
- **Train**: 9 games (Game1, 2, 3, 5, 6, 7, 8, 9, 10) ‚Üí 13,132 messages
- **Validation**: 1 game (Game11) ‚Üí 1,416 messages  
- **Test**: 2 games (Game4, 12) ‚Üí 2,741 messages

**Why Game-Level Splits:**
```
‚úì Prevents data leakage (no mixing of conversations from same game)
‚úì Realistic evaluation (testing on completely unseen games)
‚úì Preserves game context (all 21 player-pair conversations stay together)
```

**POC Implementation Note:** 
The current POC pipeline processes only the Train split (13,132 messages). This was sufficient for proof-of-concept but the full dataset with Validation and Test splits is available for more comprehensive evaluation.

**Benefits:**
- Clean separation between splits
- No data leakage
- Each game has 21 conversations (7 choose 2 player pairs)

**Alternative (If Needed):** Auto-create stratified splits within Train data for cross-validation

### 3. Label Normalization

**Problem:** ConvoKit uses "Lie"/"Truth" strings

**Decision:** Convert to binary integers (0/1)

```python
def normalize_speaker_intention(intention: str) -> int:
    if intention.lower() == "lie":
        return 1
    elif intention.lower() == "truth":
        return 0
    else:
        return None  # Skip invalid labels
```

**Benefits:**
- Standard ML format
- Compatible with scikit-learn
- Clear numeric encoding

### 4. Move Matching Strategy

**Problem:** `game_id` column is NaN in ConvoKit data

**Decision:** Extract game_id from sender format

```python
# Sender format: "italy-Game1"
if '-Game' in sender:
    game_id = int(sender.split('-Game')[1])
    country = sender.split('-')[0].lower()
```

**Solution for Move Lookup:**
```python
# Try multiple seasons (season not always available)
for season in ['spring', 'fall', 'winter']:
    key = (game_id, year, season, country)
    if key in move_index:
        return move_index[key]
```

**Result:** 99.97% match rate

### 5. Handling Class Imbalance

**Problem:** Lies are rare (4.5%) ‚Üí 21:1 imbalance

**Decision:** Use balanced class weights in LogisticRegression

```python
LogisticRegression(class_weight='balanced')
# Automatically weights samples: w_i = n_samples / (n_classes * n_samples_i)
```

**Alternative Considered:** SMOTE (synthetic oversampling) - not used to preserve natural distribution

### 6. Embedding Strategy

**Decision:** Concatenate [CLS] token + mean pooling

```python
# [CLS] token (768-dim): Sentence-level representation
cls_embedding = bert_output.last_hidden_state[:, 0, :]

# Mean pooling (768-dim): Token-level aggregation
mean_embedding = masked_mean(bert_output.last_hidden_state, attention_mask)

# Concatenate (1536-dim): Best of both
combined = np.concatenate([cls_embedding, mean_embedding], axis=1)
```

**Rationale:**
- [CLS]: BERT's sentence embedding
- Mean: Captures all token information
- Combined: Best performance (73.90% accuracy)

---

## Data Challenges & Solutions

### Challenge 1: Tuple Keys in JSON

**Problem:** Python tuples can't be JSON keys

```python
# This doesn't work:
moves_data = {
    (1, '1901', 'spring'): {...}  # Can't serialize!
}
```

**Solution:** Convert to strings when saving, parse back when loading

```python
# Save:
moves_data_str = {str(k): v for k, v in moves_data.items()}
json.dump(moves_data_str, f)

# Load:
moves_data = {eval(k): v for k, v in moves_data_str.items()}
```

### Challenge 2: Missing Receiver Annotations

**Problem:** ~8.4% of messages lack receiver_perception

```python
receiver_perception: Truth/Lie/NaN
deception_quadrant: Unknown for missing annotations
```

**Solution:** Track separately, don't use for supervised learning on suspected lies

### Challenge 3: ConvoKit API Changes

**Problem:** `conversation.get_participant_ids()` doesn't exist

**Solution:** Iterate over utterances to find speakers

```python
speakers = set()
for utterance_id in conversation.get_utterance_ids():
    utterance = corpus.get_utterance(utterance_id)
    speakers.add(utterance.speaker.id)
```

### Challenge 4: Heterogeneous Data Types

**Problem:** Mixed types in game_score_delta

```python
game_score_delta: sometimes string, sometimes int
```

**Solution:** Explicit type conversion with error handling

```python
if game_score_delta is not None:
    try:
        game_score_delta = int(game_score_delta)
    except (ValueError, TypeError):
        game_score_delta = None
```

### Challenge 5: Memory Efficiency

**Problem:** 13K √ó 1536-dim embeddings = ~80MB per embedding type

**Solution:**
- Save as compressed NumPy arrays (.npy)
- Load only needed embeddings
- Batch processing for BERT inference

```python
np.save('embeddings.npy', embeddings)  # Compressed by default
embeddings = np.load('embeddings.npy')  # Memory-mapped option available
```

---

## Data Processing Best Practices

### 1. Logging

Every step logs progress and issues:

```python
logger.info(f"Loaded {len(df)} messages")
logger.warning(f"Missing moves for {missing_count} messages")
```

### 2. Validation

Check data integrity at each step:

```python
assert len(embeddings) == len(df), "Embedding count mismatch!"
assert df['label'].isin([0, 1]).all(), "Invalid labels found!"
```

### 3. Reproducibility

Fixed random seeds throughout:

```python
random_state=42  # For train_test_split
torch.manual_seed(42)  # For BERT
```

### 4. Intermediate Outputs

Save results at each step:

```
poc/data/
‚îú‚îÄ‚îÄ cleaned_messages.csv      ‚Üê Step 1 output
‚îú‚îÄ‚îÄ moves_data.json           ‚Üê Step 1 output
‚îú‚îÄ‚îÄ message_embeddings.npy    ‚Üê Step 3 output
‚îî‚îÄ‚îÄ enriched_embeddings.npy   ‚Üê Step 4 output
```

### 5. Error Handling

Graceful degradation:

```python
try:
    moves = find_moves_for_message(message)
except Exception as e:
    logger.warning(f"Failed to find moves: {e}")
    moves = None  # Continue with None
```

---

## Example: End-to-End Data Flow

Let's trace a single message through the entire pipeline:

### Input: Raw ConvoKit Utterance

```python
utterance = corpus.get_utterance("Game1-italy-germany-74")

utterance.text = "Germany! Just the person I want to speak with..."
utterance.speaker.id = "italy-Game1"
utterance.meta = {
    'speaker_intention': 'Truth',
    'receiver_perception': 'Truth',
    'year': 1901,
    'game_score': 3,
    'game_score_delta': 0,
    'deception_quadrant': 'Straightforward'
}
```

### Step 1: Extract and Clean

```python
message_data = {
    'message': "Germany! Just the person I want to speak with...",
    'sender': 'italy-Game1',
    'receiver': 'germany-Game1',
    'label': 0,  # Truth ‚Üí 0
    'year': 1901,
    'game_score': 3,
    'game_score_delta': 0,
    'deception_quadrant': 'Straightforward',
    'split': 'Train'
}
```

### Step 2: Calculate Metrics

This message contributes to:
- Overall stats: 1 truth message
- Year 1901 stats: 1 message
- Italy‚ÜíGermany pair stats: 1 message
- Straightforward quadrant: 1 message

### Step 3: Generate BERT Embeddings

```python
# Tokenize
tokens = tokenizer("Germany! Just the person...")
# Shape: [1, seq_len] e.g., [1, 15]

# BERT forward pass
outputs = bert_model(**tokens)
# Shape: [1, seq_len, 768]

# Extract [CLS] and mean pooling
cls_emb = outputs.last_hidden_state[:, 0, :]  # [1, 768]
mean_emb = masked_mean(outputs.last_hidden_state)  # [1, 768]

# Concatenate
final_emb = np.concatenate([cls_emb, mean_emb])  # [1, 1536]
```

### Step 4: Enrich with Context

```python
# Lookup moves
game_id = 1  # From "italy-Game1"
moves = move_index[(1, '1901', 'spring', 'italy')]
# {
#   'Ven': {'type': 'HOLD', 'result': 'SUCCEEDS'},
#   'Nap': {'to': 'ION', 'type': 'MOVE', 'result': 'SUCCEEDS'},
#   'Rom': {'to': 'Apu', 'type': 'MOVE', 'result': 'SUCCEEDS'}
# }

# Construct enriched prompt
prompt = f"""Message: Germany! Just the person I want to speak with...

Player Actions: Held Ven (SUCCEEDS); Moved Nap to ION (SUCCEEDS); Moved Rom to Apu (SUCCEEDS)

Game State: Sender has 3 supply centers. Power difference: 0

Context: This message was sent during 1901 spring turn."""

# Generate enriched embedding
enriched_emb = bert_model(tokenizer(prompt))  # [1, 1536]
```

### Step 5: Use for Prediction

```python
# Load trained probe
probe = joblib.load('lie_detector_probe_enriched.pkl')

# Predict
prediction = probe.predict(enriched_emb)  # 0 (Truth)
probability = probe.predict_proba(enriched_emb)  # [0.92, 0.08]

# Result: 92% confident it's truthful (correct!)
```

---

## Summary

### Data Strengths
‚úÖ **Rich annotations**: Dual perspectives (sender + receiver)  
‚úÖ **Contextual**: Full game state available  
‚úÖ **Naturalistic**: Real strategic communication  
‚úÖ **Well-structured**: Clean extraction pipeline  
‚úÖ **High coverage**: 99.97% message-to-move matching  

### Data Challenges
‚ö†Ô∏è **Severe class imbalance**: 4.5% lies requires careful handling  
‚ö†Ô∏è **Missing annotations**: 8.4% lack receiver perception  
‚ö†Ô∏è **Complex formats**: Multiple data sources to integrate  
‚ö†Ô∏è **Incomplete splits**: ConvoKit splits needed reconstruction  

### Processing Achievements
üéØ **17,289 messages available** in ConvoKit corpus  
üéØ **13,132 Train messages processed** in POC with full metadata  
üéØ **1,787 move contexts** indexed and matched  
üéØ **Game-level splits** maintained (9/1/2 games in Train/Val/Test)  
üéØ **BERT embeddings** generated for Train messages  
üéØ **Enriched prompts** constructed with game context  

### Key Takeaways

1. **Data quality is excellent** - minimal missing data, rich annotations
2. **Integration is complex** - three data sources require careful alignment
3. **Class imbalance is the main challenge** - careful handling required
4. **Context matters** - enriched embeddings improve detection by 5.7%
5. **Processing is robust** - handles edge cases gracefully

This dataset provides a solid foundation for studying deception in strategic communication, with the POC pipeline successfully demonstrating that language embeddings can detect deceptive intent, especially when enriched with game context.

